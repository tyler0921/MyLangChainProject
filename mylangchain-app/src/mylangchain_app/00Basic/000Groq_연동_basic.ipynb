{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain!!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello LangChain!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsk_Z\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "#load_dotenv(dotenv_path='.env')\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(OPENAI_API_KEY[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt + llm + output \n",
    "\n",
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(type(prompt))\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "System: 당신은 개발자입니다.\n",
      "Human: 파이썬은 무엇인가요? 자세하게 설명해주세요\n"
     ]
    }
   ],
   "source": [
    "prompt_text = prompt.format(input=\"파이썬은 무엇인가요? 자세하게 설명해주세요\")\n",
    "print(type(prompt_text))\n",
    "print(prompt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000021A6EF568A0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021A706A3CB0> root_client=<openai.OpenAI object at 0x0000021A703E0950> root_async_client=<openai.AsyncOpenAI object at 0x0000021A704631A0> model_name='openai/gpt-oss-120b' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "#llm = ChatOpenAI(api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    #model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    #model=\"moonshotai/kimi-k2-instruct-0905\",\n",
    "    model=\"openai/gpt-oss-120b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(type(llm))\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "응답: ## 파이썬(Python)이란?\n",
      "\n",
      "파이썬은 **고수준(high‑level) 인터프리터 언어**이며, **동적 타이핑(dynamic typing)**과 **자동 메모리 관리(garbage collection)**를 제공하는 **범용 프로그래밍 언어**입니다. 1991년 네덜란드의 귀도 반 로섬(Guido van Rossum)이 처음 발표했으며, 현재는 전 세계 개발자 커뮤니티와 기업에서 가장 많이 사용되는 언어 중 하나입니다.  \n",
      "\n",
      "아래에서는 파이썬의 **역사·특징·구조·주요 활용 분야·생태계**를 상세히 살펴보겠습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 1. 파이썬의 역사와 발전\n",
      "\n",
      "| 연도 | 주요 사건 |\n",
      "|------|-----------|\n",
      "| **1980년대 후반** | 귀도 반 로섬이 “ABC” 언어의 개선판을 구상. |\n",
      "| **1991년** | 파이썬 0.9.0 공개 (유닉스, 윈도우, 매크로 언어). |\n",
      "| **1994년** | 파이썬 1.0 발표 – 모듈, 예외 처리, 함수형 프로그래밍 요소 추가. |\n",
      "| **2000년** | 파이썬 2.0 출시 – 리스트 컴프리헨션, 가비지 컬렉션, 유니코드 지원. |\n",
      "| **2008년** | 파이썬 3.0 (Python 3000) 발표 – 문자열/바이트 구분, `print` 함수화 등 비호환적 변화. |\n",
      "| **2020년** | 파이썬 3.9, 3.10, 3.11 등 최신 버전에서 성능 개선(패턴 매칭, JIT-like 최적화) 및 새로운 표준 라이브러리 추가. |\n",
      "| **2023년** | 파이썬 3.12 발표 – 파싱 속도 2배 이상 향상, 새로운 `tomllib` 등. |\n",
      "| **2025년 현재** | 파이썬 3.13이 베타 단계에 있으며, **CPython**(C 구현)과 **PyPy**(JIT 구현), **MicroPython**, **RustPython** 등 다양한 구현체가 활발히 유지·보수 중. |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. 파이썬의 핵심 특징\n",
      "\n",
      "| 특징 | 설명 | 장점 |\n",
      "|------|------|------|\n",
      "| **읽기 쉬운 문법** | 들여쓰기(Indentation)로 블록을 구분하고, 불필요한 구분자를 최소화. | 코드 가독성·유지보수성 향상 |\n",
      "| **동적 타이핑** | 변수 선언 시 타입을 명시하지 않으며, 런타임에 타입이 결정. | 빠른 프로토타이핑, 적은 보일러플레이트 |\n",
      "| **인터프리터 언어** | 소스 코드를 바로 실행(인터프리터)하거나 바이트코드(.pyc)로 컴파일 후 실행. | 플랫폼 독립성, REPL(대화형 쉘) 활용 가능 |\n",
      "| **풍부한 표준 라이브러리** | `os`, `json`, `datetime`, `http`, `sqlite3` 등 200여 개 모듈 제공. | 외부 패키지 없이도 다양한 작업 수행 가능 |\n",
      "| **확장성** | C/C++(CPython), Java(Jython), .NET(IronPython) 등으로 구현된 인터프리터와 연동 가능. | 성능-critical 부분을 C로 구현하거나 기존 시스템과 통합 용이 |\n",
      "| **멀티패러다임** | 절차적·객체지향·함수형 프로그래밍 지원. | 문제에 맞는 스타일 선택 가능 |\n",
      "| **가비지 컬렉션** | 자동 메모리 관리(참조 카운팅 + 사이클 탐지). | 메모리 누수 위험 감소 |\n",
      "| **대규모 커뮤니티와 에코시스템** | PyPI(Python Package Index)에 400,000+ 패키지 존재. | 거의 모든 분야(웹, 과학, AI, 자동화 등)에서 솔루션 찾기 쉬움 |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. 파이썬 언어 구조와 기본 문법\n",
      "\n",
      "### 3‑1. 기본 데이터 타입\n",
      "\n",
      "| 타입 | 리터럴 예시 | 특징 |\n",
      "|------|------------|------|\n",
      "| `int` | `42`, `-7` | 무제한 정밀도 정수 |\n",
      "| `float` | `3.14`, `-0.001` | IEEE 754 배정밀도 부동소수점 |\n",
      "| `bool` | `True`, `False` | 논리값 |\n",
      "| `str` | `'hello'`, `\"world\"` | 유니코드 문자열 (Python 3) |\n",
      "| `list` | `[1, 2, 3]` | 가변 순서 컬렉션 |\n",
      "| `tuple` | `(1, 2, 3)` | 불변 순서 컬렉션 |\n",
      "| `set` | `{1, 2, 3}` | 중복 없는 무순서 컬렉션 |\n",
      "| `dict` | `{'a': 1, 'b': 2}` | 키‑값 매핑, 해시 기반 |\n",
      "\n",
      "### 3‑2. 제어 흐름\n",
      "\n",
      "```python\n",
      "# 조건문\n",
      "if x > 0:\n",
      "    print(\"양수\")\n",
      "elif x == 0:\n",
      "    print(\"0\")\n",
      "else:\n",
      "    print(\"음수\")\n",
      "\n",
      "# 반복문\n",
      "for i in range(5):\n",
      "    print(i)          # 0~4 출력\n",
      "\n",
      "while condition:\n",
      "    # 반복 수행\n",
      "    ...\n",
      "\n",
      "# 리스트 컴프리헨션\n",
      "squares = [x*x for x in range(10) if x % 2 == 0]\n",
      "```\n",
      "\n",
      "### 3‑3. 함수와 람다\n",
      "\n",
      "```python\n",
      "def fib(n: int) -> int:\n",
      "    \"\"\"n번째 피보나치 수 반환 (재귀)\"\"\"\n",
      "    if n <= 1:\n",
      "        return n\n",
      "    return fib(n-1) + fib(n-2)\n",
      "\n",
      "# 람다식 (익명 함수)\n",
      "add = lambda a, b: a + b\n",
      "```\n",
      "\n",
      "### 3‑4. 클래스와 객체지향\n",
      "\n",
      "```python\n",
      "class Animal:\n",
      "    def __init__(self, name: str):\n",
      "        self.name = name\n",
      "\n",
      "    def speak(self):\n",
      "        raise NotImplementedError\n",
      "\n",
      "class Dog(Animal):\n",
      "    def speak(self):\n",
      "        return \"멍멍\"\n",
      "\n",
      "class Cat(Animal):\n",
      "    def speak(self):\n",
      "        return \"야옹\"\n",
      "\n",
      "pets = [Dog(\"바둑이\"), Cat(\"나비\")]\n",
      "for p in pets:\n",
      "    print(p.name, \":\", p.speak())\n",
      "```\n",
      "\n",
      "### 3‑5. 모듈·패키지·가상 환경\n",
      "\n",
      "```bash\n",
      "# 패키지 설치 (pip)\n",
      "pip install requests\n",
      "\n",
      "# 가상 환경 생성·활성화\n",
      "python -m venv .venv\n",
      "source .venv/bin/activate   # Linux/macOS\n",
      ".\\.venv\\Scripts\\activate    # Windows\n",
      "```\n",
      "\n",
      "```python\n",
      "# mypkg/__init__.py\n",
      "def hello():\n",
      "    print(\"Hello from my package!\")\n",
      "\n",
      "# 사용\n",
      "import mypkg\n",
      "mypkg.hello()\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "## 4. 파이썬 실행 방식\n",
      "\n",
      "1. **소스 파일(.py)** → 파이썬 인터프리터가 **파싱 → AST(Abstract Syntax Tree) 생성 → 바이트코드 컴파일**  \n",
      "2. 바이트코드(.pyc)는 **CPython 가상 머신**에 의해 **스택 기반 명령어**로 실행.  \n",
      "3. **JIT 구현체(PyPy)**는 바이트코드를 런타임에 **네이티브 머신 코드**로 변환해 성능을 크게 향상시킴.  \n",
      "\n",
      "> **핵심 포인트**: 파이썬은 **컴파일 언어**와 **인터프리터 언어**의 중간 형태(바이트코드 기반)이며, 이는 **플랫폼 독립성**과 **빠른 개발 속도**를 동시에 제공한다는 장점이 있다.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. 주요 활용 분야\n",
      "\n",
      "| 분야 | 대표 라이브러리·프레임워크 | 사용 사례 |\n",
      "|------|--------------------------|-----------|\n",
      "| **웹 개발** | Django, Flask, FastAPI, Tornado | 웹사이트·REST API·마이크로서비스 |\n",
      "| **데이터 과학·분석** | NumPy, pandas, SciPy, Dask | 데이터 전처리·통계·시계열 분석 |\n",
      "| **머신러닝·딥러닝** | scikit-learn, TensorFlow, PyTorch, Keras | 모델 학습·예측·컴퓨터 비전·자연어 처리 |\n",
      "| **자동화·스크립팅** | `os`, `subprocess`, `shutil`, `pyautogui` | 파일·시스템 관리·테스트 자동화·RPA |\n",
      "| **시스템 관리·DevOps** | Ansible, SaltStack, Fabric | 인프라 프로비저닝·배포 파이프라인 |\n",
      "| **과학·공학** | SymPy, matplotlib, Plotly, Jupyter | 수식 기호 연산·시뮬레이션·시각화 |\n",
      "| **게임·멀티미디어** | Pygame, Panda3D, Kivy | 2D/3D 게임·인터랙티브 앱 |\n",
      "| **임베디드·IoT** | MicroPython, CircuitPython | 마이크로컨트롤러(ESP32, Raspberry Pi Pico) 프로그래밍 |\n",
      "| **교육** | Turtle, IDLE, Jupyter Notebook | 프로그래밍 입문·코딩 교육 |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. 파이썬 생태계와 도구\n",
      "\n",
      "| 구분 | 도구·플랫폼 | 설명 |\n",
      "|------|------------|------|\n",
      "| **패키지 관리** | `pip`, `conda` | PyPI와 Anaconda 레포지터리에서 패키지 설치·관리 |\n",
      "| **가상 환경** | `venv`, `virtualenv`, `conda env` | 프로젝트별 의존성 격리 |\n",
      "| **IDE/편집기** | PyCharm, VS Code, Spyder, JupyterLab, Thonny | 정적 분석·디버깅·자동 완성 지원 |\n",
      "| **테스팅** | `unittest`, `pytest`, `nose2` | 단위·통합·시스템 테스트 프레임워크 |\n",
      "| **형식 검사** | `mypy`, `pyright`, `pylint`, `flake8` | 정적 타입 체크·코드 스타일 검사 |\n",
      "| **배포** | `setuptools`, `wheel`, `twine`, `poetry` | 패키지 빌드·배포·버전 관리 |\n",
      "| **CI/CD** | GitHub Actions, GitLab CI, Travis CI, CircleCI | 자동 빌드·테스트·배포 파이프라인 |\n",
      "| **문서화** | Sphinx, MkDocs, pdoc | API 문서·사용자 가이드 자동 생성 |\n",
      "\n",
      "---\n",
      "\n",
      "## 7. 성능 최적화와 한계\n",
      "\n",
      "| 상황 | 최적화 방법 | 비고 |\n",
      "|------|------------|------|\n",
      "| **CPU‑bound 연산** | C 확장 모듈 (`Cython`, `Numba`), `multiprocessing`(프로세스 기반 병렬) | GIL(Global Interpreter Lock) 회피 |\n",
      "| **I/O‑bound 작업** | `asyncio`, `aiohttp`, `trio` 같은 비동기 프레임워크 | 이벤트 루프 기반 동시성 |\n",
      "| **대규모 데이터** | `Dask`, `PySpark`, `Modin` 등 분산 데이터프레임 | 메모리 사용량 최소화 |\n",
      "| **실행 속도** | `PyPy`(JIT), `CPython` 최신 버전(3.11+), `pyston` 등 | 대부분의 경우 2~4배 가속 |\n",
      "| **배포 크기** | `pyinstaller`, `cx_Freeze`, `briefcase` 등으로 독립 실행 파일 생성 | 종속성 포함 배포 용이 |\n",
      "\n",
      "> **주의**: 파이썬은 **가독성·생산성**을 중시하는 언어이며, **실시간·초저지연 시스템(예: 고주파 트레이딩, 커널 모듈)**에는 C/C++·Rust 등 저수준 언어가 더 적합합니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 8. 파이썬 학습 로드맵 (초급 → 고급)\n",
      "\n",
      "1. **기초**  \n",
      "   - 변수·자료형·연산자  \n",
      "   - 제어문·함수·예외 처리  \n",
      "   - 기본 입출력·파일 I/O  \n",
      "\n",
      "2. **심화**  \n",
      "   - 객체지향 프로그래밍(OOP)  \n",
      "   - 모듈·패키지·가상 환경 관리  \n",
      "   - 표준 라이브러리 활용 (`datetime`, `pathlib`, `logging` 등)  \n",
      "\n",
      "3. **전문 분야**  \n",
      "   - **웹**: Flask → Django → FastAPI (비동기)  \n",
      "   - **데이터**: NumPy → pandas → matplotlib/Seaborn → scikit-learn  \n",
      "   - **AI**: TensorFlow/Keras → PyTorch → Hugging Face Transformers  \n",
      "   - **자동화**: `requests`, `beautifulsoup4`, `selenium`  \n",
      "\n",
      "4. **품질·운영**  \n",
      "   - 테스트(`pytest`), 타입 힌트(`mypy`), 코드 스타일(`black`, `flake8`)  \n",
      "   - CI/CD 파이프라인 구축  \n",
      "   - 컨테이너화(Docker)·클라우드 배포(AWS Lambda, GCP Cloud Functions)  \n",
      "\n",
      "5. **고급 주제**  \n",
      "   - 메타프로그래밍(데코레이터·클래스 메타)  \n",
      "   - C 확장·Cython·Numba  \n",
      "   - 비동기 프로그래밍(`asyncio`, `trio`)  \n",
      "   - 성능 프로파일링(`cProfile`, `line_profiler`)  \n",
      "\n",
      "---\n",
      "\n",
      "## 9. 파이썬을 선택해야 하는 이유 (요약)\n",
      "\n",
      "| 장점 | 구체적인 이유 |\n",
      "|------|----------------|\n",
      "| **가독성** | 명확한 문법·일관된 스타일 → 팀 협업 효율 상승 |\n",
      "| **생산성** | 짧은 코드·풍부한 라이브러리 → 프로토타입 제작 속도 빠름 |\n",
      "| **다양한 도메인** | 웹·데스크톱·데이터·AI·IoT 등 거의 모든 분야 커버 |\n",
      "| **활발한 커뮤니티** | 질문·답변·패키지·컨퍼런스가 풍부 → 문제 해결이 쉬움 |\n",
      "| **플랫폼 독립성** | Windows, macOS, Linux, Android, iOS(예: Kivy) 등 어디서든 실행 |\n",
      "| **오픈소스** | 무료·오픈소스 → 비용 절감·커스터마이징 자유 |\n",
      "\n",
      "---\n",
      "\n",
      "## 10. 마무리 & 참고 자료\n",
      "\n",
      "- **공식 문서**: <https://docs.python.org/3/> (영문) / <https://docs.python.org/ko/3/> (한글 번역)  \n",
      "- **학습 사이트**:  \n",
      "  - Python 공식 튜토리얼  \n",
      "  - Real Python (https://realpython.com)  \n",
      "  - Codecademy, Coursera, Udemy 등  \n",
      "- **핵심 서적**:  \n",
      "  - “파이썬 코딩 도장” – 초급 입문자용  \n",
      "  - “Fluent Python” – 고급 파이썬 테크닉  \n",
      "  - “Effective Python” – 베스트 프랙티스 90가지  \n",
      "- **커뮤니티**:  \n",
      "  - Stack Overflow (파이썬 태그)  \n",
      "  - Reddit r/Python, Discord 파이썬 서버  \n",
      "  - 국내: 파이썬·코딩·프로그래밍 카페, OKKY 등  \n",
      "\n",
      "> **한 줄 정리**: 파이썬은 **읽기 쉬운 문법 + 방대한 라이브러리**를 바탕으로 **다양한 분야에서 빠르게 문제를 해결**할 수 있게 해 주는 현대 프로그래밍 언어이며, 지속적인 업데이트와 활발한 커뮤니티 덕분에 앞으로도 중요한 위치를 유지할 것입니다.  \n",
      "\n",
      "궁금한 점이 있으면 언제든 물어보세요! 🚀\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LCEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n",
      "first=ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]) middle=[ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x0000021A6EF568A0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000021A706A3CB0>, root_client=<openai.OpenAI object at 0x0000021A703E0950>, root_async_client=<openai.AsyncOpenAI object at 0x0000021A704631A0>, model_name='openai/gpt-oss-120b', temperature=0.7, model_kwargs={}, openai_api_key=SecretStr('**********'), openai_api_base='https://api.groq.com/openai/v1')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "chain = prompt | llm | output_parser\n",
    "print(type(chain))\n",
    "print(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain.invoke({\"input\":\"LangChain은 무엇인가요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "**LangChain(랭체인)**은 **대규모 언어 모델(LLM)**을 **애플리케이션 수준**으로 활용할 수 있게 도와주는 **오픈소스 프레임워크**입니다.  \n",
      "LLM 자체는 “텍스트를 생성·예측”하는 엔진에 불과하지만, 실제 서비스에서는 다음과 같은 복합적인 작업 흐름이 필요합니다.\n",
      "\n",
      "| 필요 기능 | LangChain이 제공하는 핵심 컴포넌트 |\n",
      "|----------|-----------------------------------|\n",
      "| **프롬프트 관리** | `PromptTemplate`, `FewShotPromptTemplate` 등으로 재사용 가능한 프롬프트를 정의·조합 |\n",
      "| **체인(Chain)** | 여러 LLM 호출·전처리·후처리를 순차·조건부로 연결 (예: `LLMChain`, `SequentialChain`, `RouterChain`) |\n",
      "| **에이전트(Agent)** | 목표에 맞는 도구(tool)를 동적으로 선택·실행 (예: `ZeroShotAgent`, `ReactAgent`) |\n",
      "| **메모리(Memory)** | 대화·작업 이력 보관 (예: `ConversationBufferMemory`, `VectorStoreRetrieverMemory`) |\n",
      "| **도구·통합** | 검색 엔진, 데이터베이스, 파일 시스템, API 등 외부 서비스와 연결 (예: `Tool`, `Retriever`) |\n",
      "| **벡터 스토어** | 임베딩 기반 검색·유사도 매칭 (FAISS, Pinecone, Weaviate 등) |\n",
      "| **배포·운영** | `LangChainHub`(프롬프트·체인 공유), `LangServe`(REST API로 쉽게 배포) 등 |\n",
      "\n",
      "---\n",
      "\n",
      "## 1. 왜 LangChain이 필요한가?\n",
      "\n",
      "1. **LLM을 단일 호출이 아닌 파이프라인**으로 사용하고 싶다.  \n",
      "2. **프롬프트를 체계적으로 관리·버전 관리**하고 싶다.  \n",
      "3. **외부 데이터(검색 결과, DB, 파일 등)와 결합**한 복합 작업을 구현하고 싶다.  \n",
      "4. **대화 흐름·상태(메모리)**를 유지하면서 연속적인 질문‑응답을 제공하고 싶다.  \n",
      "5. **다양한 LLM 제공자(OpenAI, Anthropic, Cohere, Llama, HuggingFace 등)**를 추상화된 인터페이스 하나로 교체하고 싶다.\n",
      "\n",
      "LangChain은 위 요구를 **컴포넌트 단위로 분리**하고, **Python·JavaScript·TypeScript·Java·Go 등 여러 언어**에서 사용할 수 있게 구현되었습니다. 가장 활발히 사용되는 버전은 **Python**이며, 공식 문서는 <https://python.langchain.com> 에서 확인할 수 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. 주요 개념과 흐름\n",
      "\n",
      "### 2.1 PromptTemplate\n",
      "```python\n",
      "from langchain.prompts import PromptTemplate\n",
      "\n",
      "template = \"\"\"You are a helpful assistant.\n",
      "User: {question}\n",
      "Assistant:\"\"\"\n",
      "prompt = PromptTemplate.from_template(template)\n",
      "```\n",
      "- 변수(`{question}`)를 동적으로 채워서 프롬프트를 생성합니다.\n",
      "- `FewShotPromptTemplate`을 사용하면 예시(샘플)도 자동 삽입 가능.\n",
      "\n",
      "### 2.2 LLM & LLMChain\n",
      "```python\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.chains import LLMChain\n",
      "\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "chain = LLMChain(llm=llm, prompt=prompt)\n",
      "\n",
      "answer = chain.run({\"question\": \"한국의 수도는?\"})\n",
      "```\n",
      "- `LLM` 객체는 실제 모델 호출을 담당하고, `LLMChain`은 프롬프트와 LLM을 묶어 한 번에 실행합니다.\n",
      "\n",
      "### 2.3 Retriever + VectorStore\n",
      "```python\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.retrievers import VectorStoreRetriever\n",
      "\n",
      "emb = OpenAIEmbeddings()\n",
      "docs = [\"서울은 한국의 수도이다.\", \"부산은 한국의 제2도시이다.\"]\n",
      "vectorstore = FAISS.from_texts(docs, emb)\n",
      "retriever = VectorStoreRetriever(vectorstore=vectorstore, k=2)\n",
      "```\n",
      "- 문서들을 임베딩해 벡터 스토어에 저장하고, 질의와 가장 유사한 문서를 `retriever`가 반환합니다.\n",
      "- `RetrievalQAChain` 같은 체인에 연결하면 “검색 + 답변 생성” 흐름을 구현할 수 있습니다.\n",
      "\n",
      "### 2.4 Agent\n",
      "```python\n",
      "from langchain.agents import initialize_agent, Tool\n",
      "from langchain.tools import DuckDuckGoSearchRun\n",
      "\n",
      "search = DuckDuckWebSearchRun()\n",
      "tools = [Tool(name=\"search\", func=search.run, description=\"Web search\")]\n",
      "agent = initialize_agent(tools, llm, agent_type=\"zero-shot-react-description\")\n",
      "agent.run(\"2024년 한국의 인구는 얼마인가?\")\n",
      "```\n",
      "- 에이전트는 **목표**를 받아서 **어떤 도구를 언제 사용할지** 스스로 판단합니다.\n",
      "- `react`, `openai-functions`, `tool-calling` 등 다양한 전략이 제공됩니다.\n",
      "\n",
      "### 2.5 Memory\n",
      "```python\n",
      "from langchain.memory import ConversationBufferMemory\n",
      "\n",
      "memory = ConversationBufferMemory()\n",
      "chat_chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
      "\n",
      "chat_chain.run({\"question\": \"오늘 날씨 어때?\"})\n",
      "chat_chain.run({\"question\": \"내가 방금 물어본 거 기억해?\"})\n",
      "```\n",
      "- 대화 기록을 자동으로 프롬프트에 포함해 연속적인 컨텍스트를 유지합니다.\n",
      "- `ConversationSummaryMemory`, `VectorStoreRetrieverMemory` 등 목적에 맞는 여러 메모리 구현이 있습니다.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. 실제 사용 사례\n",
      "\n",
      "| 분야 | 구체적인 예시 |\n",
      "|------|--------------|\n",
      "| **챗봇** | 고객 문의 → 검색·DB 조회 → 답변 생성 + 대화 메모리 |\n",
      "| **문서 요약** | 대용량 PDF/텍스트 → chunk → 임베딩 → 유사도 검색 → 요약 LLM |\n",
      "| **코드 어시스턴트** | 사용자 코드 스니펫 → 정적 분석 → LLM에게 “버그 찾기” 요청 |\n",
      "| **비즈니스 인텔리전스** | 재무 보고서 → 벡터 검색 → “2023년 매출 추세는?” 질문 |\n",
      "| **자동화 워크플로** | “내일 회의 일정 잡아줘” → 캘린더 API 호출 → 확인 메일 전송 |\n",
      "| **교육·학습** | 교재 내용 → 퀴즈 생성 → 학습자 응답 → 맞춤형 피드백 제공 |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. 간단한 엔드‑투‑엔드 예시 (Python)\n",
      "\n",
      "아래 코드는 **“문서 검색 + 질문‑답변”** 파이프라인을 구현한 최소 예시입니다.\n",
      "\n",
      "```python\n",
      "# 1️⃣ 라이브러리 설치\n",
      "# pip install langchain openai faiss-cpu tiktoken\n",
      "\n",
      "from langchain.llms import OpenAI\n",
      "from langchain.embeddings import OpenAIEmbeddings\n",
      "from langchain.vectorstores import FAISS\n",
      "from langchain.prompts import PromptTemplate\n",
      "from langchain.chains import RetrievalQAChain\n",
      "\n",
      "# 2️⃣ LLM·임베딩 초기화\n",
      "llm = OpenAI(model=\"gpt-4o-mini\")\n",
      "emb = OpenAIEmbeddings()\n",
      "\n",
      "# 3️⃣ 문서 로드 & 벡터스토어 생성\n",
      "texts = [\n",
      "    \"서울은 대한민국의 수도이며, 한강을 끼고 있다.\",\n",
      "    \"부산은 남해안에 위치한 항구 도시이다.\",\n",
      "    \"제주도는 화산섬이며, 관광 명소가 많다.\"\n",
      "]\n",
      "vectorstore = FAISS.from_texts(texts, emb)\n",
      "\n",
      "# 4️⃣ Retriever 정의\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
      "\n",
      "# 5️⃣ 프롬프트 템플릿 (검색 결과를 LLM에 전달)\n",
      "qa_prompt = PromptTemplate.from_template(\n",
      "    \"\"\"아래는 사용자가 질문한 내용과 관련된 문서 조각들이다.\n",
      "문서:\\n{context}\\n\\n질문: {question}\\n답변을 한국어로, 가능한 한 구체적으로 작성해라.\"\"\"\n",
      ")\n",
      "\n",
      "# 6️⃣ RetrievalQAChain 구성\n",
      "qa_chain = RetrievalQAChain.from_chain_type(\n",
      "    llm=llm,\n",
      "    chain_type=\"stuff\",               # “stuff”는 모든 문서를 한 번에 LLM에 전달\n",
      "    retriever=retriever,\n",
      "    return_source_documents=True,\n",
      "    combine_prompt=qa_prompt,\n",
      ")\n",
      "\n",
      "# 7️⃣ 질문 실행\n",
      "query = \"한국의 수도는 어디인가?\"\n",
      "result = qa_chain({\"query\": query})\n",
      "\n",
      "print(\"답변:\", result[\"answer\"])\n",
      "print(\"\\n사용된 문서:\", [doc.page_content for doc in result[\"source_documents\"]])\n",
      "```\n",
      "\n",
      "**동작 흐름**  \n",
      "1. 텍스트를 임베딩 → FAISS에 저장  \n",
      "2. 사용자가 질문 → Retriever가 가장 유사한 2개의 문서 반환  \n",
      "3. 프롬프트에 `context`(검색 결과)와 `question`을 삽입 → LLM이 최종 답변 생성  \n",
      "\n",
      "---\n",
      "\n",
      "## 5. 시작하기 위한 체크리스트\n",
      "\n",
      "| 단계 | 내용 | 참고 자료 |\n",
      "|------|------|-----------|\n",
      "| **1. 환경 설정** | Python 3.9+, `pip install langchain openai` | 공식 설치 가이드 |\n",
      "| **2. API 키** | OpenAI, Anthropic 등 사용하고자 하는 LLM 제공자의 API 키 설정 (`export OPENAI_API_KEY=…`) | 각 제공자 문서 |\n",
      "| **3. 기본 튜토리얼** | “Hello LangChain” 예제 (LLMChain, PromptTemplate) | <https://python.langchain.com/docs/get_started/introduction> |\n",
      "| **4. Retrieval** | 벡터 스토어와 Retriever 연결 | <https://python.langchain.com/docs/modules/data_connection/vectorstores/> |\n",
      "| **5. Agent** | 도구와 에이전트 구현 (예: 웹 검색, DB 쿼리) | <https://python.langchain.com/docs/modules/agents/> |\n",
      "| **6. 배포** | `LangServe` 로 REST API 만들기 또는 `Docker` 이미지화 | <https://github.com/langchain-ai/langserve> |\n",
      "| **7. 커뮤니티** | LangChain Hub(프롬프트·체인 공유), Discord, GitHub Discussions | `hub.langchain.com` |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. 한 줄 요약\n",
      "\n",
      "> **LangChain은 LLM을 “프롬프트 → 검색/도구 → 메모리 → 출력”이라는 **모듈형 파이프라인**으로 쉽게 조립·운영**할 수 있게 해 주는 프레임워크이며, 복잡한 AI‑앱을 빠르게 프로토타이핑하고 프로덕션에 배포하도록 설계되었습니다.\n",
      "\n",
      "궁금한 점이 있으면 언제든 물어보세요! 🚀\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mylangchain-app-SBe-Yh6W-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
